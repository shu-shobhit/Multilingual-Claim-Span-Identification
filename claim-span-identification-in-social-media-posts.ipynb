{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8784753,"sourceType":"datasetVersion","datasetId":5281030}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ICPR 2024 Competition on Claim Span Identification\n\n## Disclaimer\n- The dataset may contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety\n- The dataset may identify individuals (i.e., one or more natural persons), either directly or indirectly\n- The dataset contain data that might be considered sensitive in any way (e.g., data that reveals race or ethnic origins, sexual orientations, religious beliefs, political opinions, etc.)\n\n## Data Format\nThe English and Hindi sets of the data have been split into train **(~6k samples)** and validation sets **(500 samples)**. These splits are the stored in standard JSON files are present in the ```data``` directory. You may open the ```.json``` files in any text editor to visualist the data structure.\n\nNote that the usernames have been anonimyzed by giving them unique IDs.\n\nOn loading the data file (say with ```json.load()``` in python), it will return a list of dictionaries, one for each of the data points. Each dictionary has an ***\"index\"*** key (0, 1, 2, ...) and the following two important keys: ***\"text_tokens\"*** and ***\"claims\"***.\n\n- The *\"text_tokens\"* contain a list of tokens that when joined form the text input.\n*Note that the output vectors (described below) for each data point need to be of the same size as the \"text_tokens\" list*\n\n- The *\"claims\"* again contain a list of dictionaries, one for each of the disjoint claim-spans present in the corresponding text. An empty list denotes there are no claim spans in the text.\nEach of these dictionaries contain the ***\"start\"*** and ***\"end\"*** indices of that particular claim-span in the text.\n\n*Note that the claim span-start index is inclusive, but the claim span-end index are exclusive and the indexing of tokens starts from 0. For example, if a span start and end is 3 and 7, and the text is \"I read that mrna vaccines cause cancer !\", then the claim is \"mrna vaccines cause cancer\" (i.e., consisting of the tokens indexed 3, 4, 5, 6).*\n\n\n## Output Predictions Format\nThe output predictions file should again be a ```.json``` file, containing a list of lists, one list for each of the data points. Each of the interal lists should be the same size as the corresponding *\"text_tokens\"* (**0/1** for each of the tokens). The elements marked **1** denote that the corresponding token is part of a claim-span, and **0** denotes it is *NOT* part of any claims.\n\nFor example, consider two texts --\n- *\"I  read  that  mrna  vaccines  cause  cancer  !\"*\n- *\"I  will  never  take  a  covid  vaccine  .  .  .\"*\n\nThen the output JSON-file should look something like:\n```\n[\n    [0 0 0 1 1 1 1 0],\n    [0 0 0 0 0 0 0 0 0 0]\n]\n```\n\n*Note that the number of tokens generated by a model's tokenizer may not be equal to the actual number of text tokens. You may need to convert the predicted vectors so that the final output vectors are the same size as text_tokens.*\n\n\n## Evaluation Metrics\nWe have calculated standard Macro-F1 and Jaccard scores for the individual data points, and then average them over all data points in the validation / test set.\nYou can run the following to print the scores:\n\n```python  metrics.py  <path_to_input_data_file>  <path_to_output_preds_file>```\n\n*Note: you may need to install \"numpy\" and \"scikit-learn\" libraries*.\nThe input data file should be of the same format as given, and the output preds file should be of the format as described above.\n\n\n## Scores on Validation set\nAs a baseline we used a basic [BERTforTokenClassification](https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertForTokenClassification) model which can generate a class prediction (here 0/1) for each of the tokens in the text.\n\nWe loaded the model encoder with weights from [bert-base-multilingual-uncased](https://huggingface.co/google-bert/bert-base-multilingual-uncased) AND fine-tuned it on the respective train sets for 5 epochs. This model was achieved a score of 49.71 Jaccard and 74.09 M-F1 on the English validation set; and 65.64 Jaccard and 79.19 M-F1 on the Hindi validation set\n\n\n## More Queries\nFor any further clarification, please write to [csi.icpr2024@gmail.com](mailto:csi.icpr2024@gmail.com)","metadata":{"id":"9yVpr4fkY9cy"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport json\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AdamW,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    AutoConfig,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    AutoModelForTokenClassification,\n    AutoTokenizer,\n    BertConfig,\n    BertForSequenceClassification,\n    BertModel,\n    BertPreTrainedModel,\n    BertTokenizerFast,\n    DebertaConfig,\n    DebertaModel,\n    DebertaTokenizerFast,\n    DebertaV2Config,\n    DebertaV2Model,\n    DebertaV2TokenizerFast,\n    GemmaConfig,\n    GemmaModel,\n    GemmaTokenizerFast,\n    PreTrainedModel,\n    XLMRobertaConfig,\n    XLMRobertaModel,\n    XLMRobertaTokenizerFast,\n)\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    classification_report,\n    f1_score,\n    jaccard_score,\n    precision_score,\n    recall_score,\n)\n\nfrom tqdm import tqdm","metadata":{"id":"ZqxpJ-5PsFBz","execution":{"iopub.status.busy":"2024-08-26T20:12:02.282362Z","iopub.execute_input":"2024-08-26T20:12:02.282797Z","iopub.status.idle":"2024-08-26T20:12:09.140063Z","shell.execute_reply.started":"2024-08-26T20:12:02.282751Z","shell.execute_reply":"2024-08-26T20:12:09.138907Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"id":"MKcJhP6Lr63B"}},{"cell_type":"code","source":"train_en = pd.read_json('/kaggle/input/icpr-csi/train-en.json')\nval_en = pd.read_json('/kaggle/input/icpr-csi/val-en.json')","metadata":{"id":"JWqqitG9XHkS","execution":{"iopub.status.busy":"2024-08-26T20:12:09.142526Z","iopub.execute_input":"2024-08-26T20:12:09.143720Z","iopub.status.idle":"2024-08-26T20:12:09.344574Z","shell.execute_reply.started":"2024-08-26T20:12:09.143663Z","shell.execute_reply":"2024-08-26T20:12:09.343319Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(train_en.iloc[9])","metadata":{"id":"lIP-TqJ6cHgz","outputId":"14a9e633-56ce-42ac-e8d1-9f61787bd28f","execution":{"iopub.status.busy":"2024-08-26T20:12:09.346268Z","iopub.execute_input":"2024-08-26T20:12:09.346715Z","iopub.status.idle":"2024-08-26T20:12:09.357131Z","shell.execute_reply.started":"2024-08-26T20:12:09.346664Z","shell.execute_reply":"2024-08-26T20:12:09.356015Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"index                                                        509\nclaims         [{'index': 0, 'start': 26, 'end': 33, 'terms':...\ntext_tokens    [COVID, 19, mortality, Stats, have, been, hamm...\nName: 9, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_en.iloc[9]['text_tokens'][26:33])","metadata":{"id":"TscZfWtMXP_9","outputId":"4179c87f-df18-4eaf-c1e0-2a636b256675","execution":{"iopub.status.busy":"2024-08-26T20:12:09.359593Z","iopub.execute_input":"2024-08-26T20:12:09.359928Z","iopub.status.idle":"2024-08-26T20:12:09.366974Z","shell.execute_reply.started":"2024-08-26T20:12:09.359893Z","shell.execute_reply":"2024-08-26T20:12:09.365950Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"['imprisonment', 'to', 'prove', 'living', 'in', 'abject', 'fear']\n","output_type":"stream"}]},{"cell_type":"code","source":"model_checkpoint = \"google-bert/bert-large-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)","metadata":{"id":"tcQsYques9KB","execution":{"iopub.status.busy":"2024-08-26T20:12:09.368122Z","iopub.execute_input":"2024-08-26T20:12:09.368470Z","iopub.status.idle":"2024-08-26T20:12:10.075363Z","shell.execute_reply.started":"2024-08-26T20:12:09.368417Z","shell.execute_reply":"2024-08-26T20:12:10.074164Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8f21488ede542cab58b1527f6864057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c122df0a78ae479bbc310032e5ee8264"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35cb5b16b5f24a60856dfb9369720e8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee2a5fec0dfc481abf1258ecd3fd4b4b"}},"metadata":{}}]},{"cell_type":"code","source":"def identify_problematic_chars(data, tokenizer):\n    problematic = []\n    for idx,item in tqdm(data.iterrows(), total = len(data)):\n        words = item['text_tokens']\n        for word_idx,word in enumerate(words):\n            if not tokenizer.tokenize(word):\n                problematic.append((idx,word_idx))\n    return problematic","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:10.076765Z","iopub.execute_input":"2024-08-26T20:12:10.077155Z","iopub.status.idle":"2024-08-26T20:12:10.084051Z","shell.execute_reply.started":"2024-08-26T20:12:10.077088Z","shell.execute_reply":"2024-08-26T20:12:10.083041Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_problematic = identify_problematic_chars(train_en,tokenizer)\nval_problematic = identify_problematic_chars(val_en,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:10.085331Z","iopub.execute_input":"2024-08-26T20:12:10.085749Z","iopub.status.idle":"2024-08-26T20:12:25.257005Z","shell.execute_reply.started":"2024-08-26T20:12:10.085713Z","shell.execute_reply":"2024-08-26T20:12:25.255904Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 5999/5999 [00:13<00:00, 428.56it/s]\n100%|██████████| 500/500 [00:01<00:00, 433.11it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for x,y in train_problematic:\n    train_en.iloc[x,2][y] =  tokenizer.pad_token\nfor x,y in val_problematic:\n    val_en.iloc[x,2][y] =  tokenizer.pad_token","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:25.258577Z","iopub.execute_input":"2024-08-26T20:12:25.259009Z","iopub.status.idle":"2024-08-26T20:12:25.277394Z","shell.execute_reply.started":"2024-08-26T20:12:25.258959Z","shell.execute_reply":"2024-08-26T20:12:25.276359Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def preprocess(data):\n    processed_data = []\n\n    for idx, item in tqdm(data.iterrows(), total=len(data)):\n        text_tokens = item['text_tokens']\n        claims = item['claims']\n        # Create label sequence (O: 0, B: 1, I: 2)\n        labels = [0] * len(text_tokens)  # Initialize all as 'O'\n        for claim in claims:\n            start, end = claim['start'], claim['end']\n            labels[start] = 1  # 'B' for beginning of claim if num_labels = 3 otherwise we set it to 2 if we only want Inside/Outside Tags.\n            for i in range(start + 1, end):\n                labels[i] = 2  # 'I' for inside of claim\n\n        # Tokenize without padding\n        encoded = tokenizer(\n            text_tokens,\n            is_split_into_words=True,\n            return_tensors='pt',\n            padding=False,\n            truncation=False\n        )\n\n        input_ids = encoded['input_ids'].squeeze()\n        attention_mask = encoded['attention_mask'].squeeze()\n\n        # Align labels with tokenized input\n        word_ids = encoded.word_ids()\n        aligned_labels = [-100] * len(input_ids)  # -100 is ignored by PyTorch loss functions\n\n        for i, word_id in enumerate(word_ids):\n            if word_id is not None:\n                aligned_labels[i] = labels[word_id]\n\n        processed_data.append({\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'labels': torch.tensor(aligned_labels),\n            'word_ids': word_ids  # Store word_ids for later use\n        })\n\n    return processed_data\n\nclass ClaimDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]","metadata":{"id":"GiZKqbuRq_Mi","execution":{"iopub.status.busy":"2024-08-26T20:12:25.278793Z","iopub.execute_input":"2024-08-26T20:12:25.279161Z","iopub.status.idle":"2024-08-26T20:12:25.292005Z","shell.execute_reply.started":"2024-08-26T20:12:25.279090Z","shell.execute_reply":"2024-08-26T20:12:25.290882Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_processed = preprocess(train_en)\nval_processed = preprocess(val_en)","metadata":{"id":"3SuzgRJ7teZu","execution":{"iopub.status.busy":"2024-08-26T20:12:25.296998Z","iopub.execute_input":"2024-08-26T20:12:25.297911Z","iopub.status.idle":"2024-08-26T20:12:30.549760Z","shell.execute_reply.started":"2024-08-26T20:12:25.297874Z","shell.execute_reply":"2024-08-26T20:12:30.548763Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"100%|██████████| 5999/5999 [00:04<00:00, 1237.60it/s]\n100%|██████████| 500/500 [00:00<00:00, 1280.76it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = ClaimDataset(train_processed)\nval_dataset = ClaimDataset(val_processed)","metadata":{"id":"FMCqLli3tgeV","execution":{"iopub.status.busy":"2024-08-26T20:12:30.551091Z","iopub.execute_input":"2024-08-26T20:12:30.551453Z","iopub.status.idle":"2024-08-26T20:12:30.555813Z","shell.execute_reply.started":"2024-08-26T20:12:30.551418Z","shell.execute_reply":"2024-08-26T20:12:30.554879Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_en.iloc[9,1]","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:30.557049Z","iopub.execute_input":"2024-08-26T20:12:30.557420Z","iopub.status.idle":"2024-08-26T20:12:30.574187Z","shell.execute_reply.started":"2024-08-26T20:12:30.557385Z","shell.execute_reply":"2024-08-26T20:12:30.573181Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'index': 0,\n  'start': 26,\n  'end': 33,\n  'terms': 'imprisonment to prove living in abject fear'},\n {'index': 1, 'start': 41, 'end': 44, 'terms': 'most expensive vaccine'}]"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset[9]","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:30.575713Z","iopub.execute_input":"2024-08-26T20:12:30.576051Z","iopub.status.idle":"2024-08-26T20:12:30.601573Z","shell.execute_reply.started":"2024-08-26T20:12:30.576017Z","shell.execute_reply":"2024-08-26T20:12:30.600578Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101,  2522, 17258,  2539, 13356, 26319,  2031,  2042, 25756,  2046,\n          2256,  4641,  2484,  1013,  1021,  2011,  5796,  2213,  2296,  2154,\n          2027,  3189,  2524,  1000,  8866,  1000,  2000, 16114,  2256, 10219,\n          2000,  6011,  2542,  1999, 11113, 20614,  3571,  2003,  1996,  2069,\n         21082,  4668,  1998,  2069,  1996,  2087,  6450, 17404,  2412, 14917,\n          2064,  4298,  3828,  2149,  3531,  5256,  2039,   102]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n 'labels': tensor([-100,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    1,    2,    2,    2,    2,    2,    2,\n            2,    0,    0,    0,    0,    0,    0,    0,    0,    1,    2,    2,\n            0,    0,    0,    0,    0,    0,    0,    0,    0, -100]),\n 'word_ids': [None,\n  0,\n  0,\n  1,\n  2,\n  3,\n  4,\n  5,\n  6,\n  7,\n  8,\n  9,\n  10,\n  11,\n  12,\n  13,\n  14,\n  14,\n  15,\n  16,\n  17,\n  18,\n  19,\n  20,\n  21,\n  22,\n  23,\n  24,\n  25,\n  26,\n  27,\n  28,\n  29,\n  30,\n  31,\n  31,\n  32,\n  33,\n  34,\n  35,\n  36,\n  37,\n  38,\n  39,\n  40,\n  41,\n  42,\n  43,\n  44,\n  45,\n  46,\n  47,\n  48,\n  49,\n  50,\n  51,\n  52,\n  None]}"},"metadata":{}}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef collate_fn(batch):\n    input_ids = [item['input_ids'] for item in batch]\n    attention_masks = [item['attention_mask'] for item in batch]\n    labels = [item['labels'] for item in batch]\n    word_ids = [item['word_ids'] for item in batch]\n\n    # Pad sequences\n    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n    labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_masks,\n        'labels': labels,\n        'word_ids': word_ids\n    }","metadata":{"id":"_wMOVceFtjq6","execution":{"iopub.status.busy":"2024-08-26T20:12:30.602602Z","iopub.execute_input":"2024-08-26T20:12:30.602949Z","iopub.status.idle":"2024-08-26T20:12:30.610181Z","shell.execute_reply.started":"2024-08-26T20:12:30.602916Z","shell.execute_reply":"2024-08-26T20:12:30.609154Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")","metadata":{"id":"4DPwos-VrXPg","execution":{"iopub.status.busy":"2024-08-26T20:12:30.611381Z","iopub.execute_input":"2024-08-26T20:12:30.611748Z","iopub.status.idle":"2024-08-26T20:12:30.624988Z","shell.execute_reply.started":"2024-08-26T20:12:30.611689Z","shell.execute_reply":"2024-08-26T20:12:30.624047Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Training samples: 5999\nValidation samples: 500\n","output_type":"stream"}]},{"cell_type":"code","source":"BATCH_SIZE = 16\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)","metadata":{"id":"e6iYn8NItr8k","execution":{"iopub.status.busy":"2024-08-26T20:12:30.626230Z","iopub.execute_input":"2024-08-26T20:12:30.626590Z","iopub.status.idle":"2024-08-26T20:12:30.635299Z","shell.execute_reply.started":"2024-08-26T20:12:30.626555Z","shell.execute_reply":"2024-08-26T20:12:30.634425Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\ndef calculate_f1_score(preds, labels):\n    mask = labels != -100\n    preds = preds[mask]\n    labels = labels[mask]\n    return f1_score(labels, preds, average='macro', zero_division=0)\n\ndef calculate_accuracy_score(preds, labels):\n    mask = labels != -100\n    preds = preds[mask]\n    labels = labels[mask]\n    return (preds == labels).sum().item() / len(labels) if len(labels) != 0 else 0\n\nimport torch\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm import tqdm\nimport torch.nn as nn\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\ndef train_model(model, train_dataloader, val_dataloader, device, num_labels = 3, num_epochs=3, lr=2e-5, warmup_steps=0, accumulation_steps=1):\n    optimizer = AdamW([\n        {'params': model.plm.parameters(), 'lr': lr},\n        {'params': model.classifier.parameters(), 'lr': lr * 30}\n    ])\n\n    total_steps = len(train_dataloader) * num_epochs // accumulation_steps\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n    loss_function = nn.CrossEntropyLoss(ignore_index=-100, label_smoothing=0.2)\n\n    train_losses, val_losses, train_f1s, val_f1s = [], [], [], []\n\n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss, train_f1_scores, train_acc_scores = 0.0, [], []\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        train_progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} - Training')\n\n        for step, batch in enumerate(train_progress_bar):\n            ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            logits, probs = model(ids, attention_mask=attention_mask)\n            loss = loss_function(logits.view(-1, num_labels), labels.view(-1))\n            loss = loss / accumulation_steps  # Normalize loss for gradient accumulation\n\n            loss.backward()\n\n            if (step + 1) % accumulation_steps == 0:\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n\n            train_loss += loss.item() * accumulation_steps  # Scale loss back to original\n            _, preds = torch.max(probs, dim=2)\n            \n            for pred, label in zip(preds, labels):\n                train_f1_scores.append(calculate_f1_score(pred.cpu().numpy(), label.cpu().numpy()))\n                train_acc_scores.append(calculate_accuracy_score(pred.cpu().numpy(), label.cpu().numpy()))\n\n            train_progress_bar.set_postfix({\n                'Training Loss': train_loss / (step + 1),\n                'Training F1': np.nanmean(train_f1_scores),\n                'Training Acc': np.nanmean(train_acc_scores)\n            })\n\n        avg_train_loss = train_loss / len(train_dataloader)\n        avg_train_f1 = np.nanmean(train_f1_scores)\n        avg_train_acc = np.nanmean(train_acc_scores)\n        train_losses.append(avg_train_loss)\n        train_f1s.append(avg_train_f1)\n        print(f\"Training Loss: {avg_train_loss}, F1 Score: {avg_train_f1}, Accuracy: {avg_train_acc}\")\n\n        # Validation phase\n        model.eval()\n        val_loss, val_f1_scores, val_acc_scores = 0.0, [], []\n        val_progress_bar = tqdm(val_dataloader, desc=f'Epoch {epoch+1}/{num_epochs} - Validation')\n\n        with torch.no_grad():\n            for batch in val_progress_bar:\n                ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n                logits, probs = model(ids, attention_mask=attention_mask)\n                loss = loss_function(logits.view(-1, num_labels), labels.view(-1))\n\n                val_loss += loss.item()\n                _, preds = torch.max(probs, dim=2)\n\n                for pred, label in zip(preds, labels):\n                    val_f1_scores.append(calculate_f1_score(pred.cpu().numpy(), label.cpu().numpy()))\n                    val_acc_scores.append(calculate_accuracy_score(pred.cpu().numpy(), label.cpu().numpy()))\n\n                val_progress_bar.set_postfix({\n                    'Validation Loss': val_loss / (val_progress_bar.n + 1),\n                    'Validation F1': np.nanmean(val_f1_scores),\n                    'Validation Acc': np.nanmean(val_acc_scores)\n                })\n\n        avg_val_loss = val_loss / len(val_dataloader)\n        avg_val_f1 = np.nanmean(val_f1_scores)\n        avg_val_acc = np.nanmean(val_acc_scores)\n        val_losses.append(avg_val_loss)\n        val_f1s.append(avg_val_f1)\n        print(f\"Validation Loss: {avg_val_loss}, F1 Score: {avg_val_f1}, Accuracy: {avg_val_acc}\")\n\n    return model, train_losses, val_losses, train_f1s, val_f1s","metadata":{"id":"zvXU1hmWp-8s","execution":{"iopub.status.busy":"2024-08-26T20:12:30.636821Z","iopub.execute_input":"2024-08-26T20:12:30.637237Z","iopub.status.idle":"2024-08-26T20:12:30.664348Z","shell.execute_reply.started":"2024-08-26T20:12:30.637194Z","shell.execute_reply":"2024-08-26T20:12:30.663249Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class BaseModel(PreTrainedModel):\n    def __init__(self, config, num_labels):\n        super(BaseModel, self).__init__(config)\n        self.plm = AutoModel.from_pretrained(model_checkpoint, output_hidden_states=True)\n        self.num_labels = num_labels\n        self.high_dropout = torch.nn.Dropout(0.3)\n        self.classifier = nn.Linear(config.hidden_size,self.num_labels)\n\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, ids, attention_mask):\n        outputs = self.plm(ids, attention_mask=attention_mask)\n        out = outputs.last_hidden_state\n\n        logits = torch.mean(torch.stack([\n            self.classifier(self.high_dropout(out))\n            for _ in range(8)\n        ], dim=0), dim=0)\n\n\n        probs = self.softmax(logits)\n\n        return logits, probs","metadata":{"id":"ORsB19kCuIx6","execution":{"iopub.status.busy":"2024-08-26T20:12:30.665454Z","iopub.execute_input":"2024-08-26T20:12:30.665748Z","iopub.status.idle":"2024-08-26T20:12:30.677678Z","shell.execute_reply.started":"2024-08-26T20:12:30.665717Z","shell.execute_reply":"2024-08-26T20:12:30.676731Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class BaseModelLSTM(PreTrainedModel):\n    def __init__(self, config):\n        super(BaseModelLSTM, self).__init__(config)\n        self.plm = AutoModel.from_pretrained(model_checkpoint, output_hidden_states=True)\n        self.high_dropout = torch.nn.Dropout(0.3)\n\n        # Define LSTM layers\n        self.lstm = nn.LSTM(input_size=config.hidden_size, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True)\n\n        # Define a linear layer to map LSTM output to the number of classes\n        self.classifier = nn.Linear(512 * 2, 2)  # 512 * 2 because of bidirectional LSTM\n\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, ids, attention_mask):\n        outputs = self.plm(ids, attention_mask=attention_mask)\n        out = outputs.last_hidden_state\n\n        # Apply high dropout and LSTM layers\n        out = self.high_dropout(out)\n        lstm_out, _ = self.lstm(out)\n\n        # Apply the classifier to each token\n        logits = self.classifier(lstm_out)\n\n        # Apply softmax to get probabilities for each token\n        probs = self.softmax(logits)\n\n        return logits, probs","metadata":{"id":"MsixPYxBuQxL","execution":{"iopub.status.busy":"2024-08-26T20:12:30.678861Z","iopub.execute_input":"2024-08-26T20:12:30.679190Z","iopub.status.idle":"2024-08-26T20:12:30.692207Z","shell.execute_reply.started":"2024-08-26T20:12:30.679157Z","shell.execute_reply":"2024-08-26T20:12:30.691161Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"class BaseModelCNN(BertPreTrainedModel):\n    def __init__(self, conf):\n        super(BaseModelCNN, self).__init__(conf)\n        self.plm = AutoModel.from_pretrained(model_checkpoint, output_hidden_states=True)\n        self.high_dropout = torch.nn.Dropout(0.3)\n\n        self.conv1 = nn.Conv1d(in_channels=768, out_channels=512, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv1d(in_channels=512, out_channels=256, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv1d(in_channels=256, out_channels=3, kernel_size=3, padding=1)\n\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, ids, attention_mask):\n        outputs = self.plm(ids, attention_mask=attention_mask)\n        out = outputs.last_hidden_state # Shape: (batch_size, sequence_length, hidden_size)\n\n        out = out.permute(0, 2, 1)  # Change to (batch_size, hidden_size, sequence_length)\n\n        # Apply dropout and convolutional layers\n        out = torch.mean(torch.stack([\n            self.conv3(self.relu(self.conv2(self.relu(self.conv1(self.high_dropout(out))))))\n            for _ in range(8)\n        ], dim=0), dim=0) # Ensemble averaging\n\n        out = out.permute(0, 2, 1)  # Change back to (batch_size, sequence_length, 2)\n\n        out = self.softmax(out)\n\n        return out","metadata":{"id":"mm0w2vL1uYRu","execution":{"iopub.status.busy":"2024-08-26T20:12:30.693275Z","iopub.execute_input":"2024-08-26T20:12:30.693680Z","iopub.status.idle":"2024-08-26T20:12:30.705410Z","shell.execute_reply.started":"2024-08-26T20:12:30.693629Z","shell.execute_reply":"2024-08-26T20:12:30.704388Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import logging\n\n# Get a list of all loggers\nloggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n\n# Set the logging level to ERROR for any logger related to \"transformers\"\nfor logger in loggers:\n    if \"transformers\" in logger.name.lower():\n        logger.setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:30.706652Z","iopub.execute_input":"2024-08-26T20:12:30.707034Z","iopub.status.idle":"2024-08-26T20:12:30.725767Z","shell.execute_reply.started":"2024-08-26T20:12:30.706987Z","shell.execute_reply":"2024-08-26T20:12:30.724809Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Set up the training\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nEPOCHS = 5\nLEARNING_RATE = 2e-5\nWARMUP_STEPS = 1\nACCUMULATION_STEPS = 2\n\nconfiguration = AutoConfig.from_pretrained(model_checkpoint, use_safetensors=True)\nmodel = BaseModel(configuration, 3).to(device)","metadata":{"id":"fPBW2dXXp-8s","execution":{"iopub.status.busy":"2024-08-26T20:12:30.726904Z","iopub.execute_input":"2024-08-26T20:12:30.727268Z","iopub.status.idle":"2024-08-26T20:12:45.804202Z","shell.execute_reply.started":"2024-08-26T20:12:30.727229Z","shell.execute_reply":"2024-08-26T20:12:45.803260Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d456b163214edea29b3326e3d21755"}},"metadata":{}}]},{"cell_type":"code","source":"trained_model, train_losses, val_losses, train_f1s, val_f1s = train_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    device,\n    num_labels = 3,\n    num_epochs=EPOCHS,\n    lr=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    accumulation_steps=ACCUMULATION_STEPS\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:12:45.805617Z","iopub.execute_input":"2024-08-26T20:12:45.806066Z","iopub.status.idle":"2024-08-26T20:28:56.315615Z","shell.execute_reply.started":"2024-08-26T20:12:45.806014Z","shell.execute_reply":"2024-08-26T20:28:56.314363Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 - Training: 100%|██████████| 375/375 [03:07<00:00,  2.00it/s, Training Loss=0.76, Training F1=0.489, Training Acc=0.771] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7601133948961893, F1 Score: 0.4885916722443396, Accuracy: 0.7714730761927527\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/5 - Validation: 100%|██████████| 32/32 [00:05<00:00,  5.61it/s, Validation Loss=0.732, Validation F1=0.516, Validation Acc=0.788]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7320959828794003, F1 Score: 0.515835384125386, Accuracy: 0.7877227461387426\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 - Training: 100%|██████████| 375/375 [03:08<00:00,  1.99it/s, Training Loss=0.709, Training F1=0.554, Training Acc=0.818]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.7086982873280843, F1 Score: 0.5542257979565599, Accuracy: 0.8177673480861705\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5 - Validation: 100%|██████████| 32/32 [00:05<00:00,  5.61it/s, Validation Loss=0.72, Validation F1=0.568, Validation Acc=0.8]   \n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7199755664914846, F1 Score: 0.5682780302667482, Accuracy: 0.7999620497281118\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 - Training: 100%|██████████| 375/375 [03:08<00:00,  1.99it/s, Training Loss=0.688, Training F1=0.61, Training Acc=0.836] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.688026177406311, F1 Score: 0.6101793001107689, Accuracy: 0.8364205353415347\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5 - Validation: 100%|██████████| 32/32 [00:05<00:00,  5.61it/s, Validation Loss=0.721, Validation F1=0.586, Validation Acc=0.799]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7207291703671217, F1 Score: 0.5860333654973973, Accuracy: 0.7986102581557942\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 - Training: 100%|██████████| 375/375 [03:08<00:00,  1.99it/s, Training Loss=0.671, Training F1=0.641, Training Acc=0.852]\n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.6708872640927632, F1 Score: 0.6410127681618185, Accuracy: 0.8520113252734108\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5 - Validation: 100%|██████████| 32/32 [00:05<00:00,  5.60it/s, Validation Loss=0.719, Validation F1=0.601, Validation Acc=0.805]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7189784348011017, F1 Score: 0.6005905117163747, Accuracy: 0.8053716724258224\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 - Training: 100%|██████████| 375/375 [03:08<00:00,  1.99it/s, Training Loss=0.657, Training F1=0.66, Training Acc=0.866] \n","output_type":"stream"},{"name":"stdout","text":"Training Loss: 0.656572429339091, F1 Score: 0.6599623933137496, Accuracy: 0.8655327202637692\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5 - Validation: 100%|██████████| 32/32 [00:05<00:00,  5.61it/s, Validation Loss=0.721, Validation F1=0.606, Validation Acc=0.807]","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 0.7210478130728006, F1 Score: 0.605685851467926, Accuracy: 0.8065693179059776\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Post-Processing","metadata":{"id":"Wv99Ls-nxl5Z"}},{"cell_type":"code","source":"def align_predictions(predictions, word_ids):\n    aligned_predictions = []\n    for pred, word_id in zip(predictions, word_ids):\n        word_id_to_preds = {}\n\n        # Collect predictions for each word_id\n        for p, w_id in zip(pred, word_id):\n            if w_id is not None:\n                if w_id not in word_id_to_preds:\n                    word_id_to_preds[w_id] = []\n                if p == 1 or p == 2:\n                    word_id_to_preds[w_id].append(1)\n                else:\n                    word_id_to_preds[w_id].append(0)\n\n        # Average predictions for each word_id and round\n        aligned_pred = []\n        if word_id_to_preds:  # Check if word_id_to_preds is not empty\n            max_word_id = max(word_id_to_preds.keys())\n            for i in range(max_word_id + 1):\n                if i in word_id_to_preds:\n                    avg_pred = np.mean(word_id_to_preds[i])\n                    aligned_pred.append(round(avg_pred))\n                else:\n                    aligned_pred.append(0)  # Default to 'O' for any missing word_ids\n        else:\n            aligned_pred = []  # Default to 'O' for all if no valid word_ids ( for handling examples with empty spans)\n\n        aligned_predictions.append(aligned_pred)\n    return aligned_predictions\n\n\n# Function to convert logits to predictions\ndef logits_to_predictions(logits, word_ids_batch):\n    predictions = torch.argmax(logits, dim=2).cpu().numpy()\n    aligned_predictions = align_predictions(predictions, word_ids_batch)\n    return aligned_predictions\n\n# Function to generate the output JSON file\ndef generate_output_json(predictions, output_path):\n    with open(output_path, 'w', encoding='utf-8') as file:\n        json.dump(predictions, file, indent=2)\n\n# Postprocess predictions and save to file\ndef postprocess_and_save_predictions(model, dataloader, tokenizer, device, output_path):\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Postprocessing\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            word_ids_batch = batch['word_ids']\n\n            logits, probs = model(input_ids, attention_mask)\n\n            batch_predictions = logits_to_predictions(logits, word_ids_batch)\n            predictions.extend(batch_predictions)\n\n    generate_output_json(predictions, output_path)\n\n# Function to calculate Macro-F1 and Jaccard scores\ndef evaluate(predictions, ground_truth):\n    macro_f1_scores = []\n    jaccard_scores = []\n\n    for idx ,(pred, true) in enumerate(zip(predictions, ground_truth)):\n        pred = np.array(pred)\n        true = np.array(true)\n#         print(f'Index: {idx}')\n#         print(f'Pred type: {type(pred)}, True type: {type(true)}')\n#         print(f'Pred: {pred}, True: {true}')\n\n        # Ensure the lengths match\n        if len(pred) != len(true):\n            print(idx)\n        else:\n            macro_f1 = f1_score(true, pred, average='macro',zero_division = 0)\n            jaccard = jaccard_score(true, pred, zero_division = 0)\n\n            macro_f1_scores.append(macro_f1)\n            jaccard_scores.append(jaccard)\n\n    avg_macro_f1 = np.nanmean(macro_f1_scores)\n    avg_jaccard = np.nanmean(jaccard_scores)\n\n    return avg_macro_f1, avg_jaccard\n\n# Function to extract ground truth labels from the validation data\ndef extract_ground_truth(data):\n    ground_truth = []\n    for item in data:\n        text_tokens = item['text_tokens']\n        claims = item['claims']\n        labels = [0] * len(text_tokens)  # Initialize all as 'O'\n        for claim in claims:\n            start, end = claim['start'], claim['end']\n            labels[start] = 1\n            for i in range(start + 1, end):\n                labels[i] = 1\n        ground_truth.append(labels)\n    return ground_truth\n\n# Evaluate predictions against ground truth\ndef evaluate_predictions(predictions_path, ground_truth):\n    with open(predictions_path, 'r', encoding='utf-8') as file:\n        predictions = json.load(file)\n\n    avg_macro_f1, avg_jaccard = evaluate(predictions, ground_truth)\n    print(f\"Macro-F1 Score: {avg_macro_f1:.4f}\")\n    print(f\"Jaccard Score: {avg_jaccard:.4f}\")\n    return avg_macro_f1,avg_jaccard","metadata":{"id":"2FiHKHKWp-8t","execution":{"iopub.status.busy":"2024-08-26T20:28:56.317626Z","iopub.execute_input":"2024-08-26T20:28:56.318465Z","iopub.status.idle":"2024-08-26T20:28:56.341297Z","shell.execute_reply.started":"2024-08-26T20:28:56.318398Z","shell.execute_reply":"2024-08-26T20:28:56.340328Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# req_idxs = []\n# for idx,item in val_en.iterrows():\n#     if item['claims']:\n#         req_idxs.append(idx)","metadata":{"id":"MTAccJEmp-8t","execution":{"iopub.status.busy":"2024-08-26T20:28:56.342653Z","iopub.execute_input":"2024-08-26T20:28:56.343079Z","iopub.status.idle":"2024-08-26T20:28:56.357685Z","shell.execute_reply.started":"2024-08-26T20:28:56.343029Z","shell.execute_reply":"2024-08-26T20:28:56.356670Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"ground_truth = extract_ground_truth(val_en.to_dict('records'))","metadata":{"id":"o2j11rb-p-8t","execution":{"iopub.status.busy":"2024-08-26T20:28:56.361551Z","iopub.execute_input":"2024-08-26T20:28:56.361872Z","iopub.status.idle":"2024-08-26T20:28:56.375115Z","shell.execute_reply.started":"2024-08-26T20:28:56.361838Z","shell.execute_reply":"2024-08-26T20:28:56.374039Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"output_predictions_path = 'output_predictions.json'\n\npostprocess_and_save_predictions(model, val_dataloader, tokenizer, device, output_predictions_path)","metadata":{"id":"36ZkefLCp-8t","execution":{"iopub.status.busy":"2024-08-26T20:28:56.376495Z","iopub.execute_input":"2024-08-26T20:28:56.376869Z","iopub.status.idle":"2024-08-26T20:29:01.528880Z","shell.execute_reply.started":"2024-08-26T20:28:56.376822Z","shell.execute_reply":"2024-08-26T20:29:01.527842Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Postprocessing: 100%|██████████| 32/32 [00:05<00:00,  6.25it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"avg_macro_f1, avg_jaccard = evaluate_predictions(output_predictions_path, ground_truth)","metadata":{"scrolled":true,"id":"pp-ORCX4p-8u","execution":{"iopub.status.busy":"2024-08-26T20:29:01.535488Z","iopub.execute_input":"2024-08-26T20:29:01.535853Z","iopub.status.idle":"2024-08-26T20:29:03.044695Z","shell.execute_reply.started":"2024-08-26T20:29:01.535816Z","shell.execute_reply":"2024-08-26T20:29:03.043660Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Macro-F1 Score: 0.7107\nJaccard Score: 0.4609\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# PLM + CRF Head","metadata":{"id":"TV2dCOydzEb0"}},{"cell_type":"code","source":"class PLMCRF(PreTrainedModel):\n    def __init__(self, config, num_labels=3):\n        super(PLMCRF, self).__init__(config)\n        self.config = config\n        self.num_labels = num_labels\n        self.plm = AutoModel.from_pretrained(model_checkpoint, add_pooling_layer=False)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n        self.init_weights()\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.plm(input_ids, attention_mask=attention_mask)\n        sequence_output = self.dropout(outputs[0])\n        emissions = self.classifier(sequence_output)\n\n        if labels is not None:\n            # Create a mask for non-padded positions (using attention_mask)\n            crf_mask = attention_mask.byte()\n\n            # Create a mask for non-ignored positions (where labels != -100)\n            active_loss = labels != -100\n\n            # Replace -100 with a valid label (0) just for the CRF\n            labels_for_crf = labels.clone()\n            labels_for_crf[labels == -100] = 0\n\n\n            loss = -self.crf(emissions, labels_for_crf, mask=crf_mask)\n\n\n            loss = loss * active_loss.float().sum(dim=1) / crf_mask.float().sum(dim=1)\n\n\n            loss = loss.mean()\n\n            return loss\n        else:\n\n            predictions = self.crf.decode(emissions, mask=attention_mask.byte())\n\n            padded_predictions = []\n            for pred, mask in zip(predictions, attention_mask):\n                seq_len = mask.sum().item()\n                padded_pred = pred[:seq_len] + [-100] * (mask.size(0) - seq_len)\n                padded_predictions.append(padded_pred)\n            return padded_predictions","metadata":{"id":"6VGamd7vzIG9","execution":{"iopub.status.busy":"2024-08-26T20:29:03.046252Z","iopub.execute_input":"2024-08-26T20:29:03.046696Z","iopub.status.idle":"2024-08-26T20:29:03.060311Z","shell.execute_reply.started":"2024-08-26T20:29:03.046645Z","shell.execute_reply":"2024-08-26T20:29:03.059181Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW, get_scheduler\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nimport numpy as np\n\ndef train_plmcrf_model(model, train_dataloader: DataLoader, val_dataloader: DataLoader, \n                       device: torch.device, num_epochs: int = 5, lr: float = 3e-5, \n                       warmup_steps: int = 0, accumulation_steps: int = 1, log_interval: int = 50):\n    \n    model.to(device)\n\n    # Initialize optimizer\n    optimizer = AdamW(model.parameters(), lr=lr)\n\n    # Scheduler for learning rate decay\n    num_training_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        \"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n    )\n\n    train_losses = []\n    val_losses = []\n    train_f1s = []\n    val_f1s = []\n\n    progress_bar = tqdm(range(num_training_steps))\n    for epoch in range(num_epochs):\n        model.train()\n        total_train_loss = 0\n        true_labels = []\n        pred_labels = []\n        \n        for step, batch in enumerate(train_dataloader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = outputs\n            loss = loss / accumulation_steps\n            loss.backward()\n\n            total_train_loss += loss.item()\n\n            if (step + 1) % accumulation_steps == 0:\n                optimizer.step()\n                lr_scheduler.step()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n            \n            # Accumulate predictions and labels for F1 score calculation\n            with torch.no_grad():\n                predictions = model(input_ids, attention_mask=attention_mask)\n                true_labels.extend(labels.cpu().numpy().flatten())\n                pred_labels.extend(np.concatenate(predictions).flatten())\n            \n            if step % log_interval == 0 and step > 0:\n                avg_loss = total_train_loss / log_interval\n                train_losses.append(avg_loss)\n                if true_labels and pred_labels:\n                    f1 = f1_score(true_labels, pred_labels, average='weighted')\n                else:\n                    f1 = 0.0\n                train_f1s.append(f1)\n                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step}/{len(train_dataloader)}], \"\n                      f\"Loss: {avg_loss:.4f}, Train F1: {f1:.4f}\")\n                total_train_loss = 0\n                true_labels = []\n                pred_labels = []\n        \n        # Validation phase\n        model.eval()\n        total_val_loss = 0\n        val_true_labels = []\n        val_pred_labels = []\n        \n        with torch.no_grad():\n            for batch in val_dataloader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                labels = batch['labels'].to(device)\n\n                loss = model(input_ids, attention_mask=attention_mask, labels=labels)\n                total_val_loss += loss.item()\n\n                predictions = model(input_ids, attention_mask=attention_mask)\n                val_true_labels.extend(labels.cpu().numpy().flatten())\n                val_pred_labels.extend(np.concatenate(predictions).flatten())\n        \n        avg_val_loss = total_val_loss / len(val_dataloader)\n        val_losses.append(avg_val_loss)\n        if val_true_labels and val_pred_labels:\n            val_f1 = f1_score(val_true_labels, val_pred_labels, average='weighted')\n        else:\n            val_f1 = 0.0\n        val_f1s.append(val_f1)\n        print(f\"Epoch [{epoch+1}/{num_epochs}] Validation Loss: {avg_val_loss:.4f}, Validation F1: {val_f1:.4f}\")\n\n    print(\"Training complete.\")\n    \n    return train_losses, val_losses, train_f1s, val_f1s","metadata":{"id":"dzCGoBT94o8_","execution":{"iopub.status.busy":"2024-08-26T20:29:03.061928Z","iopub.execute_input":"2024-08-26T20:29:03.062367Z","iopub.status.idle":"2024-08-26T20:29:03.084582Z","shell.execute_reply.started":"2024-08-26T20:29:03.062322Z","shell.execute_reply":"2024-08-26T20:29:03.083662Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!pip install pytorch-crf","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:29:03.086031Z","iopub.execute_input":"2024-08-26T20:29:03.086404Z","iopub.status.idle":"2024-08-26T20:29:18.214244Z","shell.execute_reply.started":"2024-08-26T20:29:03.086369Z","shell.execute_reply":"2024-08-26T20:29:18.213132Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchcrf import CRF","metadata":{"execution":{"iopub.status.busy":"2024-08-26T20:29:18.216118Z","iopub.execute_input":"2024-08-26T20:29:18.216574Z","iopub.status.idle":"2024-08-26T20:29:18.226613Z","shell.execute_reply.started":"2024-08-26T20:29:18.216528Z","shell.execute_reply":"2024-08-26T20:29:18.225304Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 1\nACCUMULATION_STEPS = 2\n\nmodel = PLMCRF(AutoConfig.from_pretrained(model_checkpoint)).to(device)","metadata":{"id":"wt2Cn7qu4_At","execution":{"iopub.status.busy":"2024-08-26T20:29:18.228095Z","iopub.execute_input":"2024-08-26T20:29:18.228490Z","iopub.status.idle":"2024-08-26T20:29:19.311388Z","shell.execute_reply.started":"2024-08-26T20:29:18.228443Z","shell.execute_reply":"2024-08-26T20:29:19.310544Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\nEPOCHS = 5\nLEARNING_RATE = 3e-5\nWARMUP_STEPS = 1\nACCUMULATION_STEPS = 2\n\nmodel = PLMCRF(AutoConfig.from_pretrained(model_checkpoint)).to(device)\n\n# Train model\ntrain_losses, val_losses, train_f1s, val_f1s = train_plmcrf_model(\n    model,\n    train_dataloader,\n    val_dataloader,\n    device,\n    num_epochs=EPOCHS,\n    lr=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    accumulation_steps=ACCUMULATION_STEPS\n)","metadata":{"id":"nqRVhHYd5BEH","execution":{"iopub.status.busy":"2024-08-26T20:29:19.312790Z","iopub.execute_input":"2024-08-26T20:29:19.313633Z","iopub.status.idle":"2024-08-26T20:53:35.342535Z","shell.execute_reply.started":"2024-08-26T20:29:19.313594Z","shell.execute_reply":"2024-08-26T20:53:35.341450Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1875 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d582e255d7cd468a95b750487b87ad9b"}},"metadata":{}},{"name":"stdout","text":"Epoch [1/5], Step [50/375], Loss: 288.0034, Train F1: 0.7862\nEpoch [1/5], Step [100/375], Loss: 227.9701, Train F1: 0.8234\nEpoch [1/5], Step [150/375], Loss: 215.7917, Train F1: 0.8447\nEpoch [1/5], Step [200/375], Loss: 206.7438, Train F1: 0.8461\nEpoch [1/5], Step [250/375], Loss: 204.3820, Train F1: 0.8505\nEpoch [1/5], Step [300/375], Loss: 184.2770, Train F1: 0.8636\nEpoch [1/5], Step [350/375], Loss: 195.8674, Train F1: 0.8542\nEpoch [1/5] Validation Loss: 372.1465, Validation F1: 0.8609\nEpoch [2/5], Step [50/375], Loss: 169.1743, Train F1: 0.8815\nEpoch [2/5], Step [100/375], Loss: 173.6890, Train F1: 0.8707\nEpoch [2/5], Step [150/375], Loss: 165.5052, Train F1: 0.8806\nEpoch [2/5], Step [200/375], Loss: 168.1667, Train F1: 0.8760\nEpoch [2/5], Step [250/375], Loss: 159.7442, Train F1: 0.8817\nEpoch [2/5], Step [300/375], Loss: 157.7958, Train F1: 0.8833\nEpoch [2/5], Step [350/375], Loss: 168.4650, Train F1: 0.8718\nEpoch [2/5] Validation Loss: 373.4070, Validation F1: 0.8656\nEpoch [3/5], Step [50/375], Loss: 131.6847, Train F1: 0.9051\nEpoch [3/5], Step [100/375], Loss: 132.8023, Train F1: 0.9043\nEpoch [3/5], Step [150/375], Loss: 146.8418, Train F1: 0.8935\nEpoch [3/5], Step [200/375], Loss: 130.0006, Train F1: 0.9048\nEpoch [3/5], Step [250/375], Loss: 123.7718, Train F1: 0.9093\nEpoch [3/5], Step [300/375], Loss: 126.7921, Train F1: 0.9060\nEpoch [3/5], Step [350/375], Loss: 131.6437, Train F1: 0.9038\nEpoch [3/5] Validation Loss: 376.2136, Validation F1: 0.8662\nEpoch [4/5], Step [50/375], Loss: 93.2374, Train F1: 0.9313\nEpoch [4/5], Step [100/375], Loss: 91.0117, Train F1: 0.9321\nEpoch [4/5], Step [150/375], Loss: 90.8776, Train F1: 0.9332\nEpoch [4/5], Step [200/375], Loss: 86.8515, Train F1: 0.9340\nEpoch [4/5], Step [250/375], Loss: 93.6120, Train F1: 0.9288\nEpoch [4/5], Step [300/375], Loss: 94.9164, Train F1: 0.9288\nEpoch [4/5], Step [350/375], Loss: 90.5065, Train F1: 0.9332\nEpoch [4/5] Validation Loss: 449.4429, Validation F1: 0.8663\nEpoch [5/5], Step [50/375], Loss: 67.8770, Train F1: 0.9431\nEpoch [5/5], Step [100/375], Loss: 68.6421, Train F1: 0.9463\nEpoch [5/5], Step [150/375], Loss: 67.2029, Train F1: 0.9443\nEpoch [5/5], Step [200/375], Loss: 65.4956, Train F1: 0.9435\nEpoch [5/5], Step [250/375], Loss: 64.3867, Train F1: 0.9460\nEpoch [5/5], Step [300/375], Loss: 62.0895, Train F1: 0.9460\nEpoch [5/5], Step [350/375], Loss: 58.9082, Train F1: 0.9492\nEpoch [5/5] Validation Loss: 521.9600, Validation F1: 0.8651\nTraining complete.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Postprocessing for model with CRF Head","metadata":{}},{"cell_type":"code","source":"def align_predictions(predictions, word_ids):\n    aligned_predictions = []\n    for pred, word_id in zip(predictions, word_ids):\n        word_id_to_preds = {}\n\n        # Collect predictions for each word_id\n        for p, w_id in zip(pred, word_id):\n            if w_id is not None:\n                if w_id not in word_id_to_preds:\n                    word_id_to_preds[w_id] = []\n                if p == 1 or p == 2:\n                    word_id_to_preds[w_id].append(1)\n                else:\n                    word_id_to_preds[w_id].append(0)\n\n        # Average predictions for each word_id and round\n        aligned_pred = []\n        if word_id_to_preds:  # Check if word_id_to_preds is not empty\n            max_word_id = max(word_id_to_preds.keys())\n            for i in range(max_word_id + 1):\n                if i in word_id_to_preds:\n                    avg_pred = np.mean(word_id_to_preds[i])\n                    aligned_pred.append(round(avg_pred))\n                else:\n                    aligned_pred.append(0)  # Default to 'O' for any missing word_ids\n        else:\n            aligned_pred = []  # Default to 'O' for all if no valid word_ids ( for handling examples with empty spans)\n\n        aligned_predictions.append(aligned_pred)\n    return aligned_predictions\n\n# Function to generate the output JSON file\ndef generate_output_json(predictions, output_path):\n    with open(output_path, 'w', encoding='utf-8') as file:\n        json.dump(predictions, file, indent=2)\n\n# Postprocess predictions and save to file\ndef postprocess_and_save_predictions(model, dataloader, tokenizer, device, output_path):\n    model.eval()\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Postprocessing\"):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            word_ids_batch = batch['word_ids']\n            labels = batch['labels'].to(device)\n            batch_predictions = model(input_ids = input_ids, attention_mask = attention_mask)\n            aligned_predictions = align_predictions(batch_predictions, word_ids_batch)\n            predictions.extend(aligned_predictions)\n\n    generate_output_json(predictions, output_path)\n\n# Function to calculate Macro-F1 and Jaccard scores\ndef evaluate(predictions, ground_truth):\n    macro_f1_scores = []\n    jaccard_scores = []\n\n    for idx ,(pred, true) in enumerate(zip(predictions, ground_truth)):\n        pred = np.array(pred)\n        true = np.array(true)\n#         print(f'Index: {idx}')\n#         print(f'Pred type: {type(pred)}, True type: {type(true)}')\n#         print(f'Pred: {pred}, True: {true}')\n\n        # Ensure the lengths match\n        if len(pred) != len(true):\n            print(idx)\n        else:\n            macro_f1 = f1_score(true, pred, average='macro',zero_division = 0)\n            jaccard = jaccard_score(true, pred, zero_division = 0)\n\n            macro_f1_scores.append(macro_f1)\n            jaccard_scores.append(jaccard)\n\n    avg_macro_f1 = np.nanmean(macro_f1_scores)\n    avg_jaccard = np.nanmean(jaccard_scores)\n\n    return avg_macro_f1, avg_jaccard\n\n# Function to extract ground truth labels from the validation data\ndef extract_ground_truth(data):\n    ground_truth = []\n    for item in data:\n        text_tokens = item['text_tokens']\n        claims = item['claims']\n        labels = [0] * len(text_tokens)  # Initialize all as 'O'\n        for claim in claims:\n            start, end = claim['start'], claim['end']\n            labels[start] = 1\n            for i in range(start + 1, end):\n                labels[i] = 1\n        ground_truth.append(labels)\n    return ground_truth\n\n# Evaluate predictions against ground truth\ndef evaluate_predictions(predictions_path, ground_truth):\n    with open(predictions_path, 'r', encoding='utf-8') as file:\n        predictions = json.load(file)\n\n    avg_macro_f1, avg_jaccard = evaluate(predictions, ground_truth)\n    print(f\"Macro-F1 Score: {avg_macro_f1:.4f}\")\n    print(f\"Jaccard Score: {avg_jaccard:.4f}\")\n    return avg_macro_f1,avg_jaccard","metadata":{"execution":{"iopub.status.busy":"2024-08-26T21:24:45.171177Z","iopub.execute_input":"2024-08-26T21:24:45.172264Z","iopub.status.idle":"2024-08-26T21:24:45.196132Z","shell.execute_reply.started":"2024-08-26T21:24:45.172206Z","shell.execute_reply":"2024-08-26T21:24:45.194939Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"output_predictions_path = 'output_predictions_CRFHead.json'\n\npostprocess_and_save_predictions(model, val_dataloader, tokenizer, device, output_predictions_path)","metadata":{"id":"GHxKDnCz5ST8","execution":{"iopub.status.busy":"2024-08-26T21:24:45.828447Z","iopub.execute_input":"2024-08-26T21:24:45.828874Z","iopub.status.idle":"2024-08-26T21:24:51.921665Z","shell.execute_reply.started":"2024-08-26T21:24:45.828835Z","shell.execute_reply":"2024-08-26T21:24:51.920553Z"},"trusted":true},"execution_count":57,"outputs":[{"output_type":"display_data","data":{"text/plain":"Postprocessing:   0%|          | 0/32 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b697f0e07e4805802fdb1fc5ecc4b6"}},"metadata":{}}]},{"cell_type":"code","source":"avg_macro_f1, avg_jaccard = evaluate_predictions(output_predictions_path, ground_truth)","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-08-26T21:24:53.953196Z","iopub.execute_input":"2024-08-26T21:24:53.953614Z","iopub.status.idle":"2024-08-26T21:24:55.588262Z","shell.execute_reply.started":"2024-08-26T21:24:53.953575Z","shell.execute_reply":"2024-08-26T21:24:55.587146Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Macro-F1 Score: 0.7137\nJaccard Score: 0.4767\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}